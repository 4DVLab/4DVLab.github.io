<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- <meta name="description"
    content="Fusing point cloud and image features to better align visual features with semantic features.">
  <meta name="keywords" content="zero-shot point cloud segmentation"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- Menu bar, can switch to other work or homepage -->

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


  <!-- Title, author and some icon(paper, code, video, etc...) -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                
                <a href="https://ren-ym.github.io/">Yiming Ren</a><sup></sup>,</span>
              <span class="author-block">
                <a href="">Xiao Han</a><sup></sup>,</span>
              <span class="author-block">
                <a href="https://afterjourney00.github.io/">Changfeng Zhao</a><sup></sup>,</span>
              <span class="author-block">
                <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/">Jingya Wang</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="http://xu-lan.com/">Lan Xu</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="http://www.yu-jingyi.com">Jingyi Yu</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="http://yuexinma.me">Yuexin Ma</a><sup></sup>,
              </span>
              </p>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup></sup>ShanghaiTech University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.17171.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.17171" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                <a href="https://youtu.be/a-msbGlQ3yc"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/4DVLab/LiveHPS"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://drive.google.com/drive/folders/1BRpczJHyzte4NNTrTZVHzU6MZu4hTbAz?usp=drive_link"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- teaser part -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/teaser.pdf"
                type="video/mp4">
      </video> -->
        <img src="./static/Livehps_pic/teaser.png" style="display: block; margin: 0 auto; ">
        <b>Figure 1. We propose a novel single-LiDAR-based approach for 3D HPS in large-scale scenarios, which is not limited to fixed studios, light conditions, and wearable devices. Our method predicts full human SMPL parameters(pose, shape, translation) from consecutive LiDAR point clouds and performs well for challenging poses and occlusion situations.
          </h2>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications. In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level Human Pose and Shape estimation without any limitation of light conditions and wearable devices. In particular, we design a distillation mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance. LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications. Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations. It consists of multi-modal and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs. Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <h2 class="title is-3">Method</h2>
          <!-- <div class="container_imgleft"> -->
            
          <img src="./static/Livehps_pic/pipeline.png">
          
          <!-- <img src="./static/pics/seg_vis.png" style="display: block; margin: 0 auto; "> -->
          <div class="content has-text-justified">

            <p>
              <b>The pipeline of LiveHPS. With sequential LiDAR point clouds as input, LiveHPS consists of three critical modules to obtain human SMPL parameters, including a point-based body tracker to distill the pose-prior information, a consecutive pose optimizer to refine the pose via utilizing joint-wise features, and a multi-head SMPL solver to regress parameters of human models.
            </p>
          </div>
          <!-- </div> -->
        </div>
      </div>
      <!--/ Method. -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <h2 class="title is-3">Dataset</h2>
          <!-- <div class="container_imgleft"> -->
            
          <img src="./static/Livehps_pic/dataset.png">
          
          <!-- <img src="./static/pics/seg_vis.png" style="display: block; margin: 0 auto; "> -->
          <div class="content has-text-justified">

            <p>
              <b>The capture systems of FreeMotion. In (a), we use a dense-camera capture system with LiDARs for accurate pose and shape capture. In (b), we set LiDARs and cameras at three views to capture human motions.
            </p>
          </div>
          <!-- </div> -->
        </div>
      </div>
      <!--/ Method. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset Struture.</h2>

<h3 id="title is-3" style="text-align:left;font-size: larger" ><b>Dataset file structure</b></h3>
<pre style="display: block; border: 1px solid #ccc; border-radius: 4px; text-align:left">
FreeMotion_Indoor
|——LiDAR_info
|   |——FM_Indoor_train.pkl
|   |   |——pc_x(Point cloud data in view x)
|   |   |——T_x(Ground truth of translation in view x)
|   |   |——shape(Ground truth of shape)
|   |   |——gt(Ground truth of SMPL local pose)
|   |   |——motion_id
|   |——FM_Indoor_test.pkl
|   |——...
|——Camera_info
|   |——camera18_train.pkl
|   |   |——shape(Ground truth of shape)
|   |   |——body_pose(Ground truth of body pose)
|   |   |——transl_cam(Ground truth of transl)
|   |   |——K(Camera calibration matrix)
|   |   |——root_pose_cam(Ground truth of global rotation)
|   |   |——motion_id
|   |   |——images
|   |   |——bbox
|   |   |——kp2d
|   |——camera18_test.pkl
|   |——...
|── images.tar.gz(Image data)
|── livehps.t7(Pretrained Model)
</pre>

<h4 id="specification" style="text-align:left"><b>Specification</b></h4>
<ol style="text-align:left ;margin-left:40px">
  <li>Point clouds are stored in LiDAR_info. Use <code>np.load(file_path, allow_pickle=True)</code> to load the file. </li>
  <li>We provide point cloud data from three perspectives at the same time.</li>
  <li>The all data is 10 fps.</li>
</ol>
<br>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"> Quantitative comparisons</h2>
          <img src="./static/Livehps_pic/cp_rs.png">
          <div class="content has-text-justified">
          </div>
        </div>
      </div>
      <!--/ Method. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">The results of LiveHPS on real-time-captured scenes</h2>
          <img src="./static/Livehps_pic/result.png" style="display: block; margin: 0 auto; ">
          <div class="content has-text-justified">
            <p>
          
            </p>
          </div>
        </div>
      </div>
      <!--/ Method. -->
    </div>
  </section>


  <!-- show bibtex -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{ren2024livehps,
        title={LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment},
        author={Ren, Yiming and Han, Xiao and Zhao, Chengfeng and Wang, Jingya and Xu, Lan and Yu, Jingyi and Ma, Yuexin},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        pages={1281--1291},
        year={2024}
      }
</code></pre>
    </div>
  </section>


  <!-- show license -->
  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Website source code based on the <a href="https://nerfies.github.io">Nerfies</a> project page.
              If you want to reuse their <a href="https://github.com/nerfies/nerfies.github.io">source code</a> , please
              credit them appropriately.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>