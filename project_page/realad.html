<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A human-like reasoning framework for end-to-end autonomous driving.">
  <meta name="keywords" content="ReAL-AD">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/IDKB_sources/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- Menu bar, can switch to other work or homepage -->

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<!-- Title, author and some icon(paper, code, video, etc...) -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://yuhanglu2000.github.io">Yuhang Lu</a><sup>1</sup>,
            <span class="author-block">
                Jiadong Tu<sup>1</sup>,</span>
            </span>
          </span>
          <span class="author-block">
            <a href="http://yuexinma.me">Yuexin Ma</a><sup>1,†</sup>,
            <span class="author-block">
              <a href="https://xingezhu.me/aboutme.html">Xinge Zhu</a><sup>2,†</sup>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ShanghaiTech University,</span>
            <span class="author-block"><sup>2</sup>The Chinese University of Hong Kong</span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>†</sup>Corresponding authors.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2507.12499"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.12499"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Github</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- teaser part -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" controls autoplay muted loop playsinline width="100%">
        <source src="./static/ReAL-AD_sources/demo.m4v" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            End-to-end autonomous driving has emerged as a promising approach to unify perception, prediction, and planning within a single framework, reducing information loss and improving adaptability. However, existing methods often rely on fixed and sparse trajectory supervision, limiting their ability to capture the hierarchical reasoning process that human drivers naturally employ. To bridge this gap, we propose ReAL-AD, a Reasoning-Augmented Learning framework that structures decision-making in autonomous driving based on the three-tier human cognitive model: <b>Driving Strategy, Driving Decision, and Driving Operation</b>, where Vision-Language Models (VLMs) are incorporated to enhance situational awareness and structured reasoning across these levels. Specifically, we introduce: (1) the <b>Strategic Reasoning Injector</b>, which formulates high-level driving strategies by interpreting complex traffic contexts from VLM-generated insights; (2) the <b>Tactical Reasoning Integrator</b>, which refines strategic intent into interpretable tactical choices such as lane changes, overtaking, and speed adjustments; and (3) the <b>Hierarchical Trajectory Decoder</b>, which progressively translates tactical decisions into precise control actions for smooth and human-like trajectory execution. Extensive evaluations show that integrating our framework improves planning accuracy and safety by over 30%, making end-to-end autonomous driving more interpretable and aligned with human-like hierarchical reasoning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <img src="./static/ReAL-AD_sources/images/teaser.png" style="display: block; margin: 0 auto; max-width: 90%; border: 1px solid #eee; padding: 10px; background: #f8f9fa;">
        <div class="content has-text-justified" style="margin-top: 18px;">
          <ul style="margin-left: 1.2em;">
            <li>
              <strong>Structural Limitations of End-to-End Systems:</strong>
              Existing end-to-end autonomous driving frameworks predominantly rely on fixed trajectory supervision, which constrains their ability to model the hierarchical reasoning processes inherent to human drivers.
            </li>
            <li>
              <strong>Interpretability Challenges:</strong>
              Current approaches lack alignment with the three-tier cognitive model observed in human driving, namely: strategic navigation, tactical maneuvers, and operational control, resulting in limited interpretability.
            </li>
            <li>
              <strong>Hierarchical Reasoning via Vision-Language Models:</strong>
              The proposed ReAL-AD framework leverages Vision-Language Models to reconstruct hierarchical reasoning, encompassing strategic decision-making, tactical planning, and trajectory decoding.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pipeline</h2>
        <img src="./static/ReAL-AD_sources/images/pipeline.png" style="display: block; margin: 0 auto; ">
        <div class="content has-text-justified">
          <b style="display:block; margin-top: 1.2em;">Overall pipeline of ReAL-AD:</b>
          <ol style="margin-top: 1em; margin-bottom: 1em;">
            <li><b>Multi-view images</b> are processed by the <b>Scene Encoder</b> to extract environmental features.</li>
            <li>The <b>Strategic Reasoning Injector</b> generates high-level driving decisions using structured prompts and utilizes them to enhance ego-query.</li>
            <li>The <b>Tactical Reasoning Integrator</b> outputs reactive- and regulatory-level command features.</li>
            <li>These features are then fed into the <b>Hierarchical Trajectory Decoder</b>, which progressively refines the latent trajectory space to generate the final planning trajectory.</li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <img src="./static/ReAL-AD_sources/images/experiments.png" style="display: block; margin: 0 auto; ">
        <div class="content has-text-justified">
          <p>
            We evaluated ReAL-AD on the <b>Bench2Drive</b> dataset, comparing it with leading methods. 
            <br><br>
            <b>Our approach achieves:</b>
            <ul style="text-align:left; margin-top:0.5em;">
              <li><b>Over 30% improvement</b> in planning accuracy and safety metrics (L2 error, collision rate) compared to strong baselines.</li>
              <li>Best-in-class performance among methods using vision-language models.</li>
              <li>Significant gains in real-world driving scores and completed routes in closed-loop tests.</li>
            </ul>
            <b>Why does ReAL-AD perform better?</b>
            <ul style="text-align:left; margin-top:0.5em;">
              <li>By introducing a human-like, hierarchical reasoning process, our model learns to make decisions more like an experienced driver.</li>
              <li>The structured integration of strategic, tactical, and operational reasoning enables better generalization and safer planning.</li>
              <li>Leveraging vision-language models enhances scene understanding and makes the decision process more interpretable.</li>
            </ul>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Visualization of VLM-generated information</h2>
        <img src="./static/ReAL-AD_sources/images/show_interpretability.png" style="display: block; margin: 0 auto; ">
        <div class="content has-text-justified">
          <p>
            <b>Visualization of VLM-generated driving strategies and tactical commands, showing their alignment with final planning.</b>
            <br><br>
            Our approach brings human-like reasoning into autonomous driving, making the decision process more transparent and easier to understand. The visualizations below highlight two challenging scenarios:
            <ul style="text-align:left; margin-top:0.5em;">
              <li><b>Scenario 1 (Left):</b> The model clearly identifies the need for a lane change and generates a command to execute it, resulting in a safe and smooth trajectory.</li>
              <li><b>Scenario 2 (Right):</b> The model detects pedestrians ahead, recommends slowing down and emergency braking, and plans a trajectory that prioritizes safety.</li>
            </ul>
            These examples demonstrate how our system’s intermediate reasoning steps—visible in the visualizations—directly influence the final driving decisions, making the entire process more interpretable and trustworthy.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>





<!-- show bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{lu2025realadhumanlikereasoningendtoend,
        title={ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving}, 
        author={Yuhang Lu and Jiadong Tu and Yuexin Ma and Xinge Zhu},
        year={2025},
        eprint={2507.12499},
        archivePrefix={arXiv},
        primaryClass={cs.RO},
        url={https://arxiv.org/abs/2507.12499}, 
      }
    </code></pre>
  </div>
</section>


<!-- show license -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> -->
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a
            href="https://nerfies.github.io">Nerfies</a>  project page. 
            If you want to reuse their <a
            href="https://github.com/nerfies/nerfies.github.io">source code</a> , please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
