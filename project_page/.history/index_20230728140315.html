<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Fusing point cloud and image features to better align visual features with semantic features.">
  <meta name="keywords" content="zero-shot point cloud segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- Menu bar, can switch to other work or homepage -->

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<!-- Title, author and some icon(paper, code, video, etc...) -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <!-- <a href="https://keunhong.com">Yuhang Lu</a><sup>1</sup>,</span> -->
              Yuhang Lu<sup>1</sup>,</span>
            <span class="author-block">
              <!-- <a href="https://utkarshsinha.com">Qi Jiang</a><sup>1</sup>,</span> -->
              Qi Jiang<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Uq2DuzkAAAAJ&hl=zh-CN">Runnan Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://cardwing.github.io">Yuenan Hou</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://xingezhu.me/aboutme.html">Xinge Zhu</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="http://yuexinma.me">Yuexin Ma</a><sup>1</sup>,
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ShanghaiTech University,</span>
            <span class="author-block"><sup>2</sup>The University of Hong Kong</span>
            <span class="author-block"><sup>2</sup>Shanghai AI Laboratory</span>
            <span class="author-block"><sup>2</sup>The Chinese University of Hong Kong</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2307.10782"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2307.10782"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/4DVLab/See_More_Know_More"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- teaser part -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/teaser.pdf"
                type="video/mp4">
      </video> -->
      <img src="./static/images/teaser.png" style="display: block; margin: 0 auto; ">
        <b>Figure 1. </b> Semantic features of objects obtained by word embedding contain rich and diverse information, including <b>appearance characteristics</b> existing 
        in images(i.e., color, light), <b>geometry and location</b> information contained in LiDAR point clouds(i.e., scale, shape), and some other non-visual
        properties(\ie, smell, weight). Previous image-based or point cloud-based zero-shot learning only considers the alignment between uni-modal 
        visual features and semantic features, where the former can just match a small subset of the latter. We propose a more effective solution for 
        zero-shot 3D segmentation by using multi-modal visual features.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Zero-shot point cloud segmentation aims to make deep models capable of recognizing novel objects in point cloud that are unseen in the training phase. 
            Recent trends favor the pipeline which transfers knowledge from seen classes with labels to unseen classes without labels. 
            They typically align visual features with semantic features obtained from word embedding by the supervision of seen classes' annotations. 
            However, point cloud contains limited information to fully match with semantic features. In fact, the rich appearance information of images is a natural 
            complement to the textureless point cloud, which is not well explored in previous literature. Motivated by this, we propose a <b>novel multi-modal 
            zero-shot learning method to better utilize the complementary information of point clouds and images for more accurate visual-semantic alignment</b>. 
            Extensive experiments are performed in two popular benchmarks, i.e., SemanticKITTI and nuScenes, and our method outperforms current SOTA methods 
            with 52% and 49% improvement on average for unseen class mIoU, respectively.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <img src="./static/images/method.png" style="display: block; margin: 0 auto; ">
        <div class="content has-text-justified">
          <p>
            <b>Figure 2. Method overview</b>  Firstly, 3D and 2D backbones <b>extract visual features</b> from LiDAR point cloud and image, while MLP extracts semantic 
            features. Secondly, for reducing the semantic-visual gap, <b>visual features and semantic features interact with each other</b> by learnable 
            projection in the SVFE module. Then, we make <b>semantic features adaptively select valuable visual features from two modalities</b> for 
            effective feature fusion in the SGVF module. Finally, we perform <b>semantic-visual feature alignment</b> for zero-shot learning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <img src="./static/images/comparison_experiments.png" style="display: block; margin: 0 auto; ">
        <div class="content has-text-justified">
          <p>
            <b>Figure 3. Comparison with state-of-the-art methods on SemanticKITTI and nuScenes datasets</b> We show the performance
            of diverse unseen-class settings introduced in Section.~\ref{sec:dataset}. Setting "0" indicates fully supervised manner. 
            "Improvement" means the percentage improvement in the metric unseen mIoU for our method relative to the previous SOTA method. 
            "Supervised" denotes that both seen and unseen classes have labels during the training of our method, which stands for the upper bound 
            for zero-shot learning performance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>


<!-- show bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{lu2023know,
      title={See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data}, 
      author={Yuhang Lu and Qi Jiang and Runnan Chen and Yuenan Hou and Xinge Zhu and Yuexin Ma},
      year={2023},
      eprint={2307.10782},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</code></pre>
  </div>
</section>


<!-- show license -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a
            href="https://nerfies.github.io">Nerfies</a>  project page. 
            If you want to reuse their <a
            href="https://github.com/nerfies/nerfies.github.io">source code</a> , please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
